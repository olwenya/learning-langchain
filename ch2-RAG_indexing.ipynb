{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0472b91b-0447-48de-832a-a6024057e6b2",
   "metadata": {},
   "source": [
    "# Indexing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e971e9b-3e6f-4bc2-b51f-847fb18257b0",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "You can load adata from files of different types\n",
    "- webpages\n",
    "- csv, json, markdown\n",
    "- pdfs\n",
    "_see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da704664-e171-444f-9739-0b0d43bdffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './test.txt'}, page_content='here is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\t\\t\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./test.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0fd75-eadc-4b65-985b-d07877e091e5",
   "metadata": {},
   "source": [
    "## Breaking down large documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5217e2ca-5618-4892-a6a9-7aa5c30c7f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader('https://www.langchain.com/')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(len(splitted_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34644a28-05a9-46bc-9a3b-7face1c886ab",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "1. we use `create_document` function when we are not splitting a list of documents that were loaded by a document loader\n",
    "2. there is no overlap because code is usually structured so there is less need to overlap\n",
    "3. there is an optional list of metadata in the `create_documents` function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902bb845-e912-4c56-b363-957ed14066e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another example splitting python code into chunks\n",
    "\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "python_code = '''\n",
    "def hello_world():\n",
    "    print('Hello, World!')\n",
    "\n",
    "# call the function\n",
    "hello_world()\n",
    "'''\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([python_code], [{'source': \"github.com/hello-word\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5abd6-775a-4938-aab1-778c0ab248b7",
   "metadata": {},
   "source": [
    "## Embedding == Converting documents to vectors \n",
    "Interface with models to generate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502016b2-6d53-4621-9590-c8478e47fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.006597376894205809, -0.008810247294604778, -0.008679674006998539, -0.01749679446220398, 0.027708983048796654]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = OpenAIEmbeddings()\n",
    "embeddings = model.embed_documents([\n",
    "    \"doc 1\",\n",
    "    \"doc 2\",\n",
    "    \"doc 3\",\n",
    "    \"hello world!\",\n",
    "])\n",
    "print(embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192ff70-efe7-41ec-b14e-4499d24061cb",
   "metadata": {},
   "source": [
    "## Storing vectors to a vectore store\n",
    "Use pgvector a  flavor of postgres database. You will need\n",
    "- a docker container pgvector/pgvector:latest\n",
    "- python package langchain-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9772350b-6d6e-40d9-87d4-18165682bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "connection_string = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "raw_documents = TextLoader('./test.txt').load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = splitter.split_documents(raw_documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = PGVector.from_documents(documents, embeddings, connection=connection_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ef550-de33-4d2c-8900-760e17104c8d",
   "metadata": {},
   "source": [
    "### An example of a similarity search\n",
    "FYI, the search string will be embedded using the embeddings model first, then searched in the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a72de8-fcbc-48e3-8c0f-642c2b426778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b3449f22-85cb-452a-85f0-3e964f60d7e9', metadata={'source': './test.txt'}, page_content='here is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line'),\n",
       " Document(id='af6cc80a-1f50-4d50-8981-0dc2d225088d', metadata={'source': './test.txt'}, page_content='here is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\t\\t\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line'),\n",
       " Document(id='a87bce53-e91d-4b92-9ca5-eb03b2413836', metadata={'source': './test.txt'}, page_content='here is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line\\n\\nhere is a sample text\\nhere is the second line\\nhere is the third line')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('query', k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d1759-872a-4d05-a451-108086adedfe",
   "metadata": {},
   "source": [
    "### Indexing documents that change\n",
    "Create a SQL Record Manager that will keep track of documents that have already been index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de23d92-8cf9-4f54-adc3-6cbb734ed52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 1: {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "\n",
    "collection_name = 'my_docs'\n",
    "namespace = 'my_docs_namespace'\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection_string,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,\n",
    "    db_url=connection_string,\n",
    ")\n",
    "\n",
    "# Create Schema if it does not exist\n",
    "record_manager.create_schema()\n",
    "\n",
    "docs = [\n",
    "    Document(page_content='who let the dogs out', metadata={\n",
    "        'id':1, 'source': 'songs.txt'\n",
    "    }),\n",
    "    Document(page_content='who let the cats out', metadata={\n",
    "        'id':2, 'source': 'songs.txt'\n",
    "    }),    \n",
    "]\n",
    "\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental', # no duplicates\n",
    "    source_id_key='source',\n",
    ")\n",
    "print(f'Index attempt 1: {index_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1711237b-8685-4408-8f95-caf7b34bf714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 2: {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "#second attempt, no changes\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental', # no duplicates\n",
    "    source_id_key='source',\n",
    ")\n",
    "print(f'Index attempt 2: {index_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768d0c35-7b19-45a0-b74d-31b15737b82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 2: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "# mutated doc!\n",
    "docs[0].page_content = 'I mutated!'\n",
    "\n",
    "#second attempt, no changes\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental', # no duplicates\n",
    "    source_id_key='source',\n",
    ")\n",
    "print(f'Index attempt 2: {index_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa25ba8-af79-406a-81bb-e825f0ad2c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain",
   "language": "python",
   "name": "learning-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
